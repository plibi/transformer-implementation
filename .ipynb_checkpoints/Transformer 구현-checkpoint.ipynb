{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3011177c",
   "metadata": {},
   "source": [
    "# Transformer 구현\n",
    " - https://wikidocs.net/31379\n",
    " - https://github.com/NLP-kr/tensorflow-ml-nlp-tf2/blob/master/6.CHATBOT/6.5.transformer.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c07311e",
   "metadata": {},
   "source": [
    "- '그것(it)'과 '동물(animal)'의 연관도를 높게 본다면, 두번째 어텐션 헤드는 '그것(it)'과 '피곤하였기 때문이다(tired)'의 연관도를 높게 볼 수 있습니다. 각 어텐션 헤드는 전부 다른 시각에서 보고있기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "525cb62f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-20T20:33:03.608265Z",
     "start_time": "2022-11-20T20:33:03.602256Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f6153a",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    " - RNN과 달리 트랜스포머의 입력은 단어 하나하나 순차적으로 넣지 않고 한번에 넣어줌\n",
    " - 따라서 입력에 순서 정보를 넣어줄 필요성이 있음\n",
    " - => 각 단어의 임베딩 벡터에 위치 정보들을 더해 모델의 입력으로 사용\n",
    " - (참고) positional encoding에 대한 설명 [링크](https://gaussian37.github.io/dl-concept-positional_encoding/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cd76fb5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-20T17:22:51.234159Z",
     "start_time": "2022-11-20T17:22:51.218230Z"
    }
   },
   "outputs": [],
   "source": [
    "# pos: 입력 문장의 길이\n",
    "# d_model: 임베딩 벡터의 차원\n",
    "def get_angles(pos, i, d_model):\n",
    "    angles = 1 / np.power(10000, (2 * i//2) / np.float32(d_model))\n",
    "    return pos * angles\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(\n",
    "        np.arange(position)[:, np.newaxis],\n",
    "        np.arange(d_model)[np.newaxis, :],\n",
    "        d_model)\n",
    "    \n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "        \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "423e2e0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-20T17:22:51.389118Z",
     "start_time": "2022-11-20T17:22:51.372163Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 50, 128), dtype=float32, numpy=\n",
       "array([[[ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00, ...,\n",
       "          1.00000000e+00,  0.00000000e+00,  1.00000000e+00],\n",
       "        [ 8.41470957e-01,  5.97375333e-01,  7.61720419e-01, ...,\n",
       "          1.00000000e+00,  1.15478200e-04,  1.00000000e+00],\n",
       "        [ 9.09297407e-01, -2.86285430e-01,  9.87046242e-01, ...,\n",
       "          9.99999940e-01,  2.30956401e-04,  1.00000000e+00],\n",
       "        ...,\n",
       "        [ 1.23573124e-01,  9.70037520e-01,  1.39920667e-01, ...,\n",
       "          9.99983013e-01,  5.42744854e-03,  9.99987245e-01],\n",
       "        [-7.68254638e-01,  7.74317265e-01, -6.63571715e-01, ...,\n",
       "          9.99982238e-01,  5.54292509e-03,  9.99986708e-01],\n",
       "        [-9.53752637e-01, -4.49214056e-02, -9.99784708e-01, ...,\n",
       "          9.99981523e-01,  5.65840164e-03,  9.99986112e-01]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# positional encoding 테스트\n",
    "pos = 50\n",
    "d_model = 128\n",
    "# (50, 1) (1, 128)\n",
    "x = positional_encoding(pos, d_model)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64cbd08",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44f8c9f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-02T23:28:30.051068Z",
     "start_time": "2022-10-02T23:28:30.037106Z"
    }
   },
   "outputs": [],
   "source": [
    "def scaledDotProductAttention(query, key, value, mask):\n",
    "    '''\n",
    "    attention weight을 계산하는 함수\n",
    "    \n",
    "    Args:\n",
    "    query: query의 shape = (batch_size, num_heads, query의 문장길이, d_model/num_head)\n",
    "    key: key의 shape : (batch_size, num_heads, key의 문장길이, d_model/num_head)\n",
    "    value: value의 shape : (batch_size, num_heads, value의 문장길이, d_model/num_head)\n",
    "    \n",
    "    d_model/num_head => d_k\n",
    "    q를 얻기위한 가중치행렬의 shape는 (embedding_size, d_k)\n",
    "    q의 shape는 (query의 문장길이, d_k)\n",
    "     \n",
    "    Returns:\n",
    "    output, attetion_weights\n",
    "    '''\n",
    "    \n",
    "    # 어텐션 스코어, Q와 K의 dot product\n",
    "    qk = tf.matmul(query, key, transpose_b=True)\n",
    "    \n",
    "    # 스케일링 (sqrt(dk)로 나눔)\n",
    "    # dk = d_model / num_head\n",
    "    scale = tf.cast(tf.shape(key)[-1], tf.float(32))\n",
    "    logits = qk / tf.math.sqrt(scale)\n",
    "    \n",
    "    # 마스킹\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "        \n",
    "    # 어텐션 weights\n",
    "    # size : (batch_size, num_head, query의 문장길이, key의 문장길이)\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "    \n",
    "    # Output\n",
    "    output = tf.matmul(attention_weight, value)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5463354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, name='multi_head_attention'):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # 나머지가 발생하면 안되기 때문에 \n",
    "        assert self.d_model % self.num_heads == 0\n",
    "        \n",
    "        # depth = d_model // num_heads (논문기준 64 = 512 // 8)\n",
    "        # deqth와 d_k는 같음\n",
    "        self.depth = self.d_model // self.num_Heads\n",
    "        \n",
    "        # WQ, WK, WV 정의\n",
    "        # => Q, K, V 행렬을 만들기 위한 가중치 행렬 (문장행렬에 가중치행렬이 곱해져 Q, K, V를 얻음)\n",
    "        self.wq = tf.keras.layers.Dense(units=d_model)\n",
    "        self.wk = tf.keras.layers.Dense(units=d_model)\n",
    "        self.wv = tf.keras.layers.Dense(units=d_model)\n",
    "        \n",
    "        # WO 정의\n",
    "        # => Attention Heads들 Concat에 곱해주는 행렬\n",
    "        self.dense = tf.keras.layer.Dense(units=d_model)\n",
    "        \n",
    "    def splitHeads(self, inputs, batch_size):\n",
    "        '''\n",
    "        num_heads 수만큼 Q, K, V를 split하는 함수\n",
    "        \n",
    "        Return:\n",
    "        (batch_size, num_heads, seq_len, depth)\n",
    "        '''\n",
    "        inputs = tf.reshape(inputs, shape=(batch_size, -1, self.num_heads, self_depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        q, k, v, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "        \n",
    "        # 1) WQ, WK, WV에 해당하는 dense layer 통과     \n",
    "        ## 인코더(k,v) - 디코더(q) 어텐션에서는 query 길이와 key, value길이가 다를 수 있다\n",
    "        query = self.wq(q)    # query : (batch_size, seq_len_q, d_model)\n",
    "        key = self.wk(k)      # key   : (batch_size, seq_len_k, d_model)\n",
    "        value = self.wv(v)    # value : (batch_size, seq_len_v, d_model)\n",
    "        \n",
    "        # 2) split head\n",
    "        query = self.splitHeads(query, batch_size)    # query : (batch_size, num_heads, seq_len_q, depth)\n",
    "        key = self.splitHeads(key, batch_size)        # key : (batch_size, num_heads, seq_len_k, depth)\n",
    "        value = self.splitHeads(value, batch_size)    # value : (batch_size, num_heads, seq_len_v, depth)\n",
    "        \n",
    "        # 3) scaled dot product attention\n",
    "        # (batch_size, num_heads, seq_len_q, d_model/num_heads)\n",
    "        scaled_attention, _ = scaledDotProductAttention(query, key, value, mask)\n",
    "        \n",
    "        # (batch_size, seq_len_q, num_heads, d_model/num_heads) => ??\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        # 4) head 연결\n",
    "        # (batch_size, seq_len_q, d_model)\n",
    "        concat_attention = tf.reshape(scaled_attention, shape=(batch_size, -1, self.d_model))\n",
    "        \n",
    "        # 5) WO에 해당하는 dense layer 통과\n",
    "        # (batch_size, seq_len_q, d_model)\n",
    "        output = self.dense(concat_attention)\n",
    "        \n",
    "        \n",
    "        return output\n",
    "\n",
    "# 1. 왜 WQ, WK ,WV를 dense layer로 통과시켜주면 되는건지\n",
    "# 2. 왜 split에서 (batch_size, seq_len, d_model)을 (batch_size, -1, num_heads, depth)로 reshape 해주는지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9c0ed2",
   "metadata": {},
   "source": [
    "### Position-wise Feed-Foward Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7d51ef94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-20T20:33:27.286397Z",
     "start_time": "2022-11-20T20:33:27.270845Z"
    }
   },
   "outputs": [],
   "source": [
    "# 논문에서 dff = 2048\n",
    "def FFN(dff, d_model):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(units=dff, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6ef1d2",
   "metadata": {},
   "source": [
    "### Residual Connection & Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8b4029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70353d03",
   "metadata": {},
   "source": [
    "### EncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1022ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EncoderLayer\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    # Add&Norm 이전 layer는 dropout\n",
    "    def __init__(self, dff, d_model, num_heads, dropout, name='encoder_layer'):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        self.ffn = FFN(dff, d_model)\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate=dropout)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate=dropout)\n",
    "    \n",
    "    # MHA - dropout - Add&Norm - FFN - dropout - Add&Norm\n",
    "    def call(self, x, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)    # x: query, key, value\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)    # x + attn_output => residual connection\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        \n",
    "        return out2\n",
    "        \n",
    "        \n",
    "# Encoder\n",
    "class Encoder(tf.keras.layer.Layer):\n",
    "    def __init__(self, dff, vocab_size, num_layers, d_model, num_heads, dropout, name='Encoder'):\n",
    "        super(Encoder, self).__init__(name=name)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vacab_size, self.d_model)\n",
    "        self.pos_encoding = positional_encoding(vocab_size, self.d_model)\n",
    "        \n",
    "        self.enc_layers = [EncoderLayer(dff=dff, d_model=self.d_model, num_heads=self.num_heads, dropout=self.dropout)\n",
    "                           for _ in range(self.num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=dropout)\n",
    "        \n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        # x = query, key, value\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, mask)\n",
    "            \n",
    "            \n",
    "        return x\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
