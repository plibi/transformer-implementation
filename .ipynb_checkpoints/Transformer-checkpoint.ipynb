{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3011177c",
   "metadata": {},
   "source": [
    "# Transformer 구현\n",
    " - https://wikidocs.net/31379\n",
    " - https://github.com/NLP-kr/tensorflow-ml-nlp-tf2/blob/master/6.CHATBOT/6.5.transformer.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c07311e",
   "metadata": {},
   "source": [
    "- '그것(it)'과 '동물(animal)'의 연관도를 높게 본다면, 두번째 어텐션 헤드는 '그것(it)'과 '피곤하였기 때문이다(tired)'의 연관도를 높게 볼 수 있습니다. 각 어텐션 헤드는 전부 다른 시각에서 보고있기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525cb62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44f8c9f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-02T23:28:30.051068Z",
     "start_time": "2022-10-02T23:28:30.037106Z"
    }
   },
   "outputs": [],
   "source": [
    "def scaledDotProductAttention(query, key, value, mask):\n",
    "    '''\n",
    "    \n",
    "    Args:\n",
    "    query: query의 size = (batch_size, num_heads, query의 문장길이, d_model/num_head)\n",
    "    key: key의 size : (batch_size, num_heads, key의 문장길이, d_model/num_head)\n",
    "    value: value의 size : (batch_size, num_heads, value의 문장길이, d_model/num_head)\n",
    "    \n",
    "    d_model/num_head => d_k\n",
    "    q를 얻기위한 가중치행렬의 size는 (embedding_size, d_k)\n",
    "    q의 size는 (query의 문장길이, d_k)\n",
    "     \n",
    "    Returns:\n",
    "    output, attetion_weights\n",
    "    '''\n",
    "    \n",
    "    # 어텐션 스코어, Q와 K의 dot product\n",
    "    qk = tf.matmul(query, key, transpose_b=True)\n",
    "    \n",
    "    # 스케일링 (sqrt(dk)로 나눔)\n",
    "    # dk = d_model / num_head\n",
    "    scale = tf.cast(tf.shape(key)[-1], tf.float(32))\n",
    "    logits = qk / tf.math.sqrt(scale)\n",
    "    \n",
    "    # 마스킹\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "        \n",
    "    # 어텐션 weights\n",
    "    # size : (batch_size, num_head, query의 문장길이, key의 문장길이)\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "    \n",
    "    # Output\n",
    "    output = tf.matmul(attention_weight, value)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5463354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.layer):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, name='multi head attention'):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        assert d_model % self.num_heads == 0\n",
    "        \n",
    "        # depth = d_model // num_heads (논문기준 512 // 8 = 64)\n",
    "        # deqth와 d_k는 같음\n",
    "        self.depth = d_model // self.num_Heads\n",
    "        \n",
    "        # WQ, WK, WV 정의\n",
    "        # => Q, K, V 행렬을 만들기 위한 가중치 행렬 (문장행렬에 가중치행렬을 곱해 Q, K, V를 얻음)\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        \n",
    "        # WO 정의\n",
    "        # => Attention Heads들 Concat에 곱해주는 행렬\n",
    "        self.dense = tf.keras.layer.Dense(units=d_model)\n",
    "        \n",
    "        \n",
    "    # num_heads 수만큼 Q, K, V를 split하는 함수\n",
    "    def splitHeads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(inputs, shape=(batch_size, -1, self.num_heads, self_depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "        \n",
    "        \n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "        \n",
    "        \n",
    "        query = self.splitHeads(query, batch_size)\n",
    "        key = self.splitHeads(key, batch_size)\n",
    "        value = self.splitHeads(value, batch_size)\n",
    "        \n",
    "        \n",
    "        scaled_attention, _ = scaledDotProductAttention(query, key, value, mask)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        \n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "        \n",
    "        \n",
    "        output = self.dense(concat_attention)\n",
    "        \n",
    "        \n",
    "        return output\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
