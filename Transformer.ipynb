{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc130e68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-19T11:19:33.112145Z",
     "start_time": "2023-03-19T11:19:33.072761Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, name='multi_head_attention'):\n",
    "        super().__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, q, k, v, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "        output = tf.matmul(attention_weights, v)\n",
    "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(output, (batch_size, -1, self.d_model))\n",
    "\n",
    "        output = self.dense(concat_attention)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "    \n",
    "def positional_encoding(position, d_model):\n",
    "    def get_angles(pos, i, d_model):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "        return pos * angle_rates\n",
    "\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "\n",
    "    # Apply sine to even indices\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # Apply cosine to odd indices\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    out = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),\n",
    "        tf.keras.layers.Dense(d_model)])\n",
    "    return out\n",
    "\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    # seq: (batch_size, seq_len)\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, dff, d_model, num_heads, dropout, name='encoder_layer'):\n",
    "        super().__init__(name=name)\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(dff, d_model)\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        x = inputs\n",
    "        \n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        \n",
    "        return out2\n",
    "\n",
    "    \n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, num_layers, dff, d_model, num_heads, dropout, name='encoder'):\n",
    "        super().__init__(name=name)\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(vocab_size, d_model)\n",
    "        \n",
    "        self.enc_layers = [EncoderLayer(dff=dff, d_model=d_model, num_heads=num_heads, dropout=dropout, name='enc_layer_{}'.format(i+1)) for i in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        # inputs shape: (batch_size, input_seq_len)\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        \n",
    "        x = self.embedding(inputs)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, dff, d_model, num_heads, dropout, name='decoder_layer'):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads, name='mha1')\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads, name='mha2')\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name='layernorm1')\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name='layernorm2')\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name='layernorm3')\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout, name='dropout1')\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout, name='dropout2')\n",
    "        self.dropout3 = tf.keras.layers.Dropout(dropout, name='dropout3')\n",
    "        \n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        x, enc_outputs, look_ahead_mask, padding_mask = inputs\n",
    "        \n",
    "        attn1, _ = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        out1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(x + out1)\n",
    "        \n",
    "        attn2, _ = self.mha2(out1, enc_outputs, enc_outputs, padding_mask)\n",
    "        out2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(out1 + out2)\n",
    "        \n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(out2 + ffn_output)  # (batch_size, target_seq_len, d_model)\n",
    "        \n",
    "        return out3\n",
    "        \n",
    "        \n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, num_layers, dff, d_model, num_heads, dropout, name='decoder'):\n",
    "        super().__init__(name=name)\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(vocab_size, d_model)\n",
    "        \n",
    "        self.dec_layers = [DecoderLayer(dff=dff, d_model=d_model, num_heads=num_heads, dropout=dropout, name=f'dec_layer_{i}') for i in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        # inputs: (dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask)\n",
    "        dec_inputs, enc_outputs, look_ahead_mask, padding_mask = inputs\n",
    "        seq_len = tf.shape(dec_inputs)[1]\n",
    "        \n",
    "        x = self.embedding(dec_inputs)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i]([x, enc_outputs, look_ahead_mask, padding_mask], training)\n",
    "        \n",
    "        return x  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, num_layers, dff, d_model, num_heads, dropout, name='transformer'):\n",
    "        super().__init__(name=name)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dff = dff\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Encoder, Decoder\n",
    "        self.encoder = Encoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff, d_model=d_model, num_heads=num_heads, dropout=dropout)\n",
    "        self.decoder = Decoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff, d_model=d_model, num_heads=num_heads, dropout=dropout)\n",
    "        \n",
    "        self.enc_padding_mask = tf.keras.layers.Lambda(create_padding_mask, output_shape=(1, 1, None), name='enc_padding_mask')\n",
    "        self.look_ahead_mask = tf.keras.layers.Lambda(create_look_ahead_mask, output_shape=(1, None, None,), name='look_ahead_mask')\n",
    "        self.dec_padding_mask = tf.keras.layers.Lambda(create_padding_mask, output_shape=(1, 1, None), name='dec_padding_mask')\n",
    "        \n",
    "        self.outputs = tf.keras.layers.Dense(units=vocab_size, name='outputs')\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        enc_inputs, dec_inputs = inputs\n",
    "        \n",
    "        enc_padding_mask = self.enc_padding_mask(enc_inputs)\n",
    "        look_ahead_mask = self.look_ahead_mask(dec_inputs)\n",
    "        dec_padding_mask = self.dec_padding_mask(dec_inputs)\n",
    "        \n",
    "        enc_outputs = self.encoder(inputs=[enc_inputs, enc_padding_mask], training=training)\n",
    "        dec_outputs = self.decoder(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask], training=training)\n",
    "        \n",
    "        outputs = self.outputs(dec_outputs)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'num_layers': self.num_layers,\n",
    "            'dff': self.dff,\n",
    "            'd_model': self.d_model,\n",
    "            'num_heads': self.num_heads,\n",
    "            'dropout': self.dropout,\n",
    "        }\n",
    "        base_config = super(Transformer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ce1d2f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-19T11:25:37.565771Z",
     "start_time": "2023-03-19T11:25:37.276343Z"
    }
   },
   "outputs": [],
   "source": [
    "transformer_model = Transformer(vocab_size=10000, num_layers=6, dff=2048, d_model=512, num_heads=8, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7281ea21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-19T11:28:48.580378Z",
     "start_time": "2023-03-19T11:28:48.573396Z"
    }
   },
   "outputs": [],
   "source": [
    "# 추후 사용시에는 아래와 같이 모듈화 해 사용하면 좋을 것 같다\n",
    "# 1. multi_head_attnetion.py\n",
    "# 2. positional_encoding.py\n",
    "# 3. encoder.py\n",
    "# 4. decoder.py\n",
    "# 5. transformer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8944b41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
